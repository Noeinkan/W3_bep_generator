
---

# STEP 1 – STRUCTURE MAPPING: BENCHMARK BEP

## Full Hierarchical Table of Contents — Benchmark BEP

**Document:** EDEU-GGP-ZZZZ-XXXXXX-PLN-IM-000001 (P04)
**Title:** EDEU BIM Execution Plan — Strategic Infrastructure Onshore Exchange
**Total Pages:** 85
**Classification:** CI-Confidential, Export Controlled

---

### Complete TOC with Page Numbers

| Section | Title | Page |
|---|---|---|
| **0** | **Document History** | **5** |
| 0.1 | Governance | 5 |
| — | RACI Review Record | 6 |
| **1** | **Introduction** | **7** |
| 1.1 | Who uses the BIM Execution Plan? | 7 |
| 1.1.1 | Task Team Definition | 8 |
| **2** | **Project Description** | **9** |
| 2.1 | Project description and purpose | 9 |
| 2.2 | Great Grid Partnership Delivery Model | 11 |
| 2.3 | Project plan of work/program/initiative | 12 |
| 2.4 | Referenced material | 13 |
| 2.5 | BEP Exceptions and Exclusions | 13 |
| **3** | **Project Team and Roles** | **15** |
| 3.1 | Project Team Organisation | 15 |
| 3.2 | Information management responsibilities | 16 |
| 3.3 | Delivery team's organisational structure and composition | 22 |
| 3.3.1 | Task Teams | 22 |
| 3.4 | Health and Safety | 23 |
| 3.4.1 | Introduction | 23 |
| 3.4.2 | CDM | 24 |
| 3.5 | Third party interface | 25 |
| 3.5.1 | Neighbouring National Grid Projects | 25 |
| 3.5.2 | Directly appointed Third party in NGSI | 25 |
| 3.5.3 | Neighbouring schemes interfacing with Project EDEU | 25 |
| **4** | **Information Delivery Strategy** | **26** |
| 4.1 | Level of Information Need | 26 |
| 4.1.1 | Geometric Information | 26 |
| 4.1.2 | Non-Geometric Information | 27 |
| 4.2 | Information Delivery Plan | 28 |
| 4.2.1 | High-level responsibility Matrix | 28 |
| 4.2.2 | Detailed Responsibility Matrix | 30 |
| 4.2.3 | Task Information delivery plan(s) | 30 |
| 4.2.4 | Master Information Delivery Plan | 31 |
| 4.3 | Client and Project IM & Digital Objectives | 31 |
| 4.3.1 | Approach to facilitating the Appointing Party's IM goals | 32 |
| 4.4 | Mobilisation plan | 32 |
| 4.5 | Delivery Team Capability & Capacity Summary | 32 |
| 4.6 | Training requirements | 33 |
| 4.7 | Delivery Team's information risk register | 33 |
| **5** | **Federation Strategy** | **34** |
| 5.1 | Federation for Coordination | 34 |
| 5.1.1 | Model Breakdown Structure | 34 |
| 5.1.2 | Federated Model Breakdown | 34 |
| 5.1.3 | Model Coordination Baseline | 34 |
| 5.1.4 | Model Federation Process | 35 |
| 5.1.5 | Federation Process Steps | 35 |
| **6** | **PIM And AIM Delivery Strategy** | **37** |
| 6.1 | Overview of Information Requirements and Delivery Model | 37 |
| 6.2 | Project Information Model delivery strategy | 37 |
| 6.2.1 | Common Data Environment – Client | 37 |
| 6.2.1.1 | Common Data Environment – Partners | 38 |
| 6.2.2 | Distributed information model | 39 |
| 6.2.3 | PIM Exchange | 39 |
| 6.2.4 | Information model Assurance | 40 |
| 6.2.4.1 | Engineering deliverable (S5) in Stage F | 40 |
| 6.2.4.2 | Non-Engineering (S5) in Stage F | 40 |
| 6.2.5 | Stage-based delivery | 40 |
| 6.2.5.1 | Security on Exchanges | 42 |
| 6.2.6 | Information Delivery Milestones | 42 |
| 6.3 | Asset Information Model Delivery Strategy | 43 |
| 6.3.1 | Scope | 43 |
| 6.3.2 | AIM diagram | 44 |
| 6.3.3 | Validation and readiness criterion | 44 |
| 6.3.3.1 | Asset Data Template (ADT) | 44 |
| 6.3.3.2 | GIS data exchange readiness | 44 |
| 6.3.3.3 | CAD deliverables | 45 |
| 6.3.3.4 | Documentation metadata and compliance | 45 |
| 6.3.3.5 | Health and Safety (BP137) | 45 |
| 6.3.3.6 | OEM, Commissioning and Technical records | 45 |
| 6.3.4 | Evidence to Demonstrate Acceptance | 45 |
| 6.3.5 | Segregation from Project Delivery and Operations | 45 |
| **7** | **Software, hardware and IT infrastructure** | **46** |
| 7.1 | Software | 46 |
| **8** | **Implementation of the Information Standard** | **47** |
| 8.1 | File Referencing and Naming Conventions | 47 |
| 8.1.1 | Information container identification | 47 |
| 8.1.1.1 | Agreed originator codes | 47 |
| 8.1.2 | Object identification (ID) – Specific to Designer | 48 |
| 8.2 | Units | 48 |
| 8.3 | Coordinates | 49 |
| 8.3.1 | Coordinate Grid Baselines | 49 |
| 8.3.2 | Project Points/Survey points (Revit) | 50 |
| 8.4 | Classification of information | 50 |
| 8.4.1 | CAD Layer Naming Strategy | 50 |
| 8.4.2 | Layer format | 50 |
| 8.4.2.1 | Layer Naming Fields | 51 |
| 8.5 | Exchange Formats and Version Compatibility | 51 |
| 8.5.1 | Agreed Exchange Formats | 51 |
| 8.5.2 | Software Version Management | 52 |
| 8.6 | Authoring Software Standardisation | 52 |
| 8.6.1 | Revit (Substations, Buildings) | 53 |
| 8.6.2 | AutoCAD/Civil 3D (Civils, Cable Routes, Site Grading) | 54 |
| 8.6.3 | PLS-CADD (OHL Design, Tower Spotting) | 55 |
| 8.6.4 | Navisworks | 55 |
| **9** | **Coordination and Clash Detection** | **56** |
| 9.1 | Model Coordination | 56 |
| 9.1.1 | ACC Model Coordination Workflow | 56 |
| 9.1.2 | Coordination Cycle | 56 |
| 9.1.3 | Federation Schedule | 57 |
| 9.1.4 | Coordination Cycle by Project Stage | 57 |
| 9.1.5 | Weekly Coordination Schedule | 57 |
| 9.2 | Clash Detection | 58 |
| 9.2.1 | Clash Detection and Resolution Responsibilities | 58 |
| 9.2.2 | Clash Detection Methodology | 58 |
| 9.2.3 | Clash Detection Rulesets | 59 |
| **10** | **Implementation of Information Production Methods and Procedures** | **61** |
| 10.1 | Information Delivery Workflows | 61 |
| 10.2 | Change in S3 – Workflow | 61 |
| 10.3 | Transmittals | 62 |
| 10.4 | Drawing types and drawing sheet templates | 63 |
| 10.4.1 | Drawing types | 63 |
| 10.4.2 | Drawing sheet templates | 63 |
| 10.5 | Modifying Existing As-built Information | 64 |
| 10.5.1 | Revising Existing Information | 65 |
| 10.5.2 | Superseding Existing Information | 65 |
| 10.5.3 | Obsoleting information | 66 |
| 10.5.4 | General note for Assurance | 67 |
| 10.6 | Security of information | 67 |
| 10.6.1 | Security of requirements | 67 |
| 10.6.1.1 | Baseline requirements | 67 |
| 10.6.1.2 | Interim Governance Position | 68 |
| 10.6.1.3 | PIM Security control | 68 |
| 10.6.1.4 | Roles and Responsibilities (Interim) | 69 |
| 10.6.1.5 | Incident management and escalation | 69 |
| 10.6.1.6 | Monitoring and Assurance | 69 |
| 10.6.1.7 | SMP Transition | 69 |
| 10.7 | Legacy information | 70 |
| 10.7.1 | Reference information, shared resources and object library | 70 |
| 10.7.2 | Agreed methods of legacy information release | 70 |
| 10.8 | Survey Strategy | 70 |
| 10.9 | RFI/TQ Process | 71 |
| 10.10 | Partner-to-Partner Communications | 71 |
| **App A** | **Abbreviations and glossary of terms** | **73** |
| **App B** | **Applicable Standards and Reference Summary** | **75** |
| **App C** | **Mobilisation Checklist** | **80** |
| **App D** | **PIM Information Breakdown Strategy** | **84** |
| **App E** | **Project Organisation and Contact Information** | **85** |

---

## STEP 2 – ATOMIC BREAKDOWN OF THE BENCHMARK BEP

Below is an atomic-level breakdown of each major section, identifying the key elements that make this document a high-quality reference.

---

### Section 0 — Document History & Governance (pp. 5–6)

**Atomic elements:**
1. **Revision table** (Table 0-1) — 4 revisions tracked with date, revision number, status code (S2/S3), author(s), and change description
2. **Contributors table** (Table 0-2) — Named individuals with company and role
3. **Governance triggers** (Table 0-1 on p.5) — 6 explicit trigger events for BEP change management, each with an accountable party
4. **RACI Review Record** (Table 0-2 on p.6) — Full RACI table listing every function/subfunction (SPIM, SBIM, SGIS, GGP IM Lead, GGP BIM Lead, GGP GIS Lead, PDD, LD, MC, PD-CDM, NG SPM, NG Project Controls, NG IM and Digital Lead) with their RACI designation, named individuals, shared flag, date, and review comments

**Quality observations:** This is exceptionally thorough. The RACI review record for the BEP itself is a best-practice element rarely seen — it proves stakeholder buy-in and provides an audit trail for the document's approval.

---

### Section 1 — Introduction (pp. 7–8)

**Atomic elements:**
1. Statement of BEP purpose — response to EIR and PIR
2. Diagram (Figure 1-1) showing relationship between PIR → EIR → BEP → Information Standard → IPMP → Mobilisation Plan/Risk Register/Information Delivery Plan
3. List of documents the BEP signposts to (IS, IPMP, MIDP, Risk Register, Mobilisation Plan, Federation Strategy)
4. Note that a BEP must be provided for each direct appointment
5. **Task Team Definition** (1.1.1) — precise ISO 19650 terminology distinguishing Task Team, Appointed Party, Lead Appointed Party, and Integrated Project Delivery Team
6. Figure 1-2 — ISO 19650-2:2018 diagram showing interfaces between parties

**Quality observations:** Clear ISO 19650 alignment with explicit traceability from client requirements to the BEP response. The relationship diagram is particularly valuable for onboarding.

---

### Section 2 — Project Description (pp. 9–14)

**Atomic elements:**
1. **Project description** — 66 km Brinsworth-High Marnham, 275kV→400kV uprating, scope of physical works (substations, OHL, towers, cabling, commissioning)
2. **Project map** (Figure 2-1)
3. **Key governance table** (Table 2-1) — Appointing Party, Project Executive, project reference/OPPM, governance stage, contract form (NEC4 Option E)
4. **Project purpose statement** — NETS reinforcement, B8 boundary, capacity/capability deficit figures, Net Zero 2035
5. **GGP Delivery Model** (2.2) — Figure 2-2 (partnership wheel), Figure 2-3 (colour-coded entity key), explanation of integrator role
6. **Scope division table** (Table 2-2) — Partner, scope summary, work packages
7. **High-level programme** (Figure 2-4)
8. **Referenced material** (2.4) — Table 2-3 with 10 key reference standards (document number, title, revision)
9. **BEP Exceptions and Exclusions** (2.5) — Three tables:
   - Table 2-4: ISO reference standard exceptions (with change, mitigation, risk register ref)
   - Table 2-5: Project reference standard exceptions (5 entries with risk register references)
   - Table 2-6: Key EDEU documentation list (10+ documents with reference, title, revision, purpose, risk/comments)

**Quality observations:** The exceptions framework is outstanding — it explicitly documents deviations from ISO 19650 and client standards with mitigations and risk register traceability. This is a hallmark of a mature BEP.

---

### Section 3 — Project Team and Roles (pp. 15–25)

**Atomic elements:**
1. **Project Team Organisation** (Table 3-1) — Full Lot 1 and Lot 2 team structures with named individuals, roles, and organisations
2. **Information management responsibilities** (Table 3-2, pp. 16–22) — 12 distinct IM functions:
   - Client (National Grid) — 10 responsibilities
   - Client Information Management — 12 responsibilities
   - Client SI Digital — 9 responsibilities
   - Client Document Control — 7 responsibilities
   - GGP Lead Information Manager — 18 responsibilities
   - GGP Lead BIM/Digital Delivery Manager — 15 responsibilities
   - GGP Lead GIS Manager — 10 responsibilities
   - GGP Lead Designer — 25+ responsibilities (very detailed, referencing specific BPs)
   - GGP Senior Project Information Manager — 20+ responsibilities
   - Senior BIM Project Coordinator — 10 responsibilities
   - Senior Project GIS Coordinator — 12 responsibilities
   - Package Information Managers — 12 responsibilities
   - Information/Document Controller — 5 responsibilities
   - Task Information Manager — 6 responsibilities
3. **Task Teams** (3.3.1) — Table 3-3 with names, digital roles, security clearance status, and contact details; Figure 3-1 org chart
4. **Health and Safety** (3.4) — ISO 19650-6:2024 aspiration, BP137 forms, CDM 2015 table (Table 3-4), risk integration into models via shared parameters
5. **Third party interfaces** (3.5) — Tables 3-5 (neighbouring NG projects), 3-6 (directly appointed third parties), 3-7 (neighbouring schemes)

**Quality observations:** The roles and responsibilities matrix is exceptionally granular — each function has between 5 and 25 individual bullet-point responsibilities. The inclusion of security clearance status is a strong compliance measure. The H&S section references ISO 19650-6:2024 which is cutting-edge.

---

### Section 4 — Information Delivery Strategy (pp. 26–33)

**Atomic elements:**
1. **Level of Information Need** (4.1) — Figure 4-1 showing Geometric (LOD), Non-Geometric (Uniclass, LOI, Attributes, Lifecycle data), Documentation (Specs, Certs, Reports)
2. **Geometric Information** (4.1.1) — Table 4-1 (LOD 2 with purpose), Figure 4-2 (visual example), NBS banding reference
3. **Non-Geometric Information** (4.1.2) — Table 4-2 (LOI 2 with purpose), Uniclass 2015, NBS LOI link
4. **High-level Responsibility Matrices** (4.2.1) — Tables 4-3, 4-4, 4-5 for Sub-50, 51, 52 respectively, mapping PIM elements across Stages E/F/G with responsible organisations
5. **Detailed Responsibility Matrix** reference (4.2.2)
6. **TIDP list** (Table 4-6) — 15 individual TIDPs with package, organisation, file name, description
7. **MIDP** reference (4.2.4)
8. **IM & Digital Objectives** (Table 4-7) — 8 strategic objectives with digital actions and responses (CDE, coordination, TIDP/MIDP, clash detection, quantities, H&S, field use, analysis)
9. **Appointing Party IM goals** (Table 4-8) — placeholder for priorities
10. **Mobilisation plan** reference (4.4)
11. **Capability & Capacity** reference (4.5)
12. **Training requirements** (4.6) — Tables 4-9 (partner WIP training, 3 modules) and 4-10 (client systems training, 8 modules) with duration, audience, delivery dates
13. **Risk register** reference (4.7)

**Quality observations:** The three separate high-level responsibility matrices per substation package, the exhaustive TIDP register, and the dual training plan tables (partner WIP + client systems) demonstrate significant planning maturity.

---

### Section 5 — Federation Strategy (pp. 34–36)

**Atomic elements:**
1. Definition of federation and its purposes
2. **Model Breakdown Structure** (5.1.1) — 3-level hierarchy (Asset → Design Package → Discipline), additional principles (Uniclass, file size, ownership)
3. **Federated Model Breakdown** (5.1.2) — 10-field Model Register template, maintenance responsibility, ACC location
4. **Model Coordination Baseline** (5.1.3) — Shared Levels and Grids model, geolocation verification requirements (4 bullet points), coordinate system cross-reference
5. **Model Federation Process** (5.1.4) — Federation responsibility assignment, single-file-per-federation (NWD/IFC2x3)
6. **Federation Process Steps** (5.1.5) — 8-step process with detailed substeps for clash detection (ACC Model Coordination + Navisworks), issue creation requirements (5 items), IPMP cross-reference
7. **Federation flowchart** (Figure 5-1) — Swimlane diagram (BIM Coordinator / SBIM / Coordination review)

**Quality observations:** The federation section is highly technical and operationally detailed. The 8-step process with ACC/Navisworks dual methodology and the swimlane flowchart are exemplary.

---

### Section 6 — PIM and AIM Delivery Strategy (pp. 37–45)

**Atomic elements:**
1. **Overview diagram** (Figure 6-1) — OIR/PIR/AIR/EIR → BEP → PIM/AIM → CDE lifecycle
2. **PIM Strategy** definition — validation, acceptance, assurance framework
3. **Client CDE** (6.2.1) — Table 6-1 (5 client CDE resources: ACC, RFIs, SharePoint, ESRI, FastDraft)
4. **Partner CDE** (6.2.1.1) — Table 6-2 (3 partners × multiple platforms)
5. **Distributed CDE diagram** (Figure 6-2) — WIP → Shared → Published across all entities
6. **PIM Exchange** (6.2.3) — WIP/Shared/Published definitions; Table 6-3 (CDE environment summary with owner, info type, permitted status, retention)
7. **Information model Assurance** (6.2.4) — WAP/DID → TIDP → Federation → Internal review → DCAAR → Assurance Review pipeline
8. **Engineering S5** (6.2.4.1) — DCAAR process, published status "An", rejection workflow
9. **Non-Engineering S5** (6.2.4.2) — Function Lead review workflow
10. **Stage-based delivery** (6.2.5) — Figure 6-3 (BP500SI Stage Gate Process); progressive assurance explanation; DID → WAP → Design Packages → TIDP → Assurance Reports pipeline; Table 6-4 (stage-by-stage assurance table for Stages E/F1/F2/G, engineering and non-engineering)
11. **Security on Exchanges** (6.2.5.1) — Table 6-5 (4 security classifications: PA/IU/CI/SC); 3-tier security control framework (Local WIP → CDE Transfer Validation → Shared/Published Controls)
12. **Information Delivery Milestones** (6.2.6) — Table 6-6 (20+ milestones across Stages E/F/G with gates, start/end dates, programme version)
13. **AIM Strategy** (6.3) — Scope referencing BP221, ECM, SR137, Ellipse
14. **AIM diagram** (Figure 6-4) — WIP → Shared → Published → GIS/ECM/Ellipse with verification loops
15. **Validation criteria** (6.3.3) — 6 subsections: ADT, GIS, CAD, documentation metadata, H&S (BP137), OEM/commissioning records — each with detailed final acceptance criteria
16. **Evidence to Demonstrate Acceptance** (6.3.4) — 6 evidence types
17. **Segregation** (6.3.5) — Project vs Operations separation

**Quality observations:** This is the most comprehensive PIM/AIM section I have seen in a BEP. The progressive assurance pipeline (DID → WAP → Design Packages → TIDP → DCAAR), the 3-tier security on exchanges, and the 6-category AIM validation criteria are all best-practice elements.

---

### Section 7 — Software, Hardware and IT Infrastructure (p. 46)

**Atomic elements:**
1. Software table (Table 7-1) — by discipline/task team with software, version, service pack
2. 2-year version update cycle policy
3. Reference to Information Standard Section 3.2

**Quality observations:** Compact but adequate. The version management cycle is well-defined.

---

### Section 8 — Implementation of the Information Standard (pp. 47–55)

**Atomic elements:**
1. **Information container ID** (8.1.1) — Table 8-1 (7-field naming convention with examples)
2. **Originator codes** (8.1.1.1) — Table 8-2 (9 codes)
3. **Object identification** (8.1.2) — Table 8-3 (ISO 22014:2024, 5-field convention with example)
4. **Units** (8.2) — Detailed prose + Table 8-4 (9 information types with unit system, symbol, notes)
5. **Coordinates** (8.3) — OSGB36, OSTN15/OSGM15, local grid for substations, similarity transformation
6. **Coordinate Grid Baselines** (8.3.1) — Table 8-5 (3 use cases: Design/Internal/OHL with grid name, CRS, purpose, notes)
7. **Survey points** (8.3.2) — Tables 8-7, 8-8, 8-9 per substation (Northing, Easting, Elevation, Rotation)
8. **Classification** (8.4) — Uniclass 2015, BS 8541-1:2012
9. **CAD Layer Naming** (8.4.1–8.4.2.1) — Convention formula, Table 8-10 (4 fields with examples)
10. **Exchange Formats** (8.5.1) — Table 8-11 (comprehensive format matrix: documentation, presentation, 2D, 3D, point clouds, federated, 4D, programme, geospatial)
11. **Software Version Management** (8.5.2) — Table 8-12 (5 software with version, format, dates)
12. **Authoring Software Standardisation** (8.6) — 4 subsections:
    - Revit (8.6.1): shared parameters (Table 6-13), family naming (Table 8-14), published views (Table 8-15), view settings (Table 8-16), file maintenance (Table 8-17)
    - AutoCAD/Civil 3D (8.6.2): layer management (Table 8-18), naming (Table 8-19), interoperability (Table 8-20)
    - PLS-CADD (8.6.3): naming (Table 8-21), layer naming (Table 8-22), data exchange (Table 8-23)
    - Navisworks (8.6.4): coordination setup, clash detection (Table 8-24), distribution

**Quality observations:** Exceptionally detailed technical standards. The per-software standardisation tables (Revit alone has 5 tables) demonstrate deep operational planning. The coordinate strategy distinguishing local grid vs internal grid vs national grid is highly professional.

---

### Section 9 — Coordination and Clash Detection (pp. 56–60)

**Atomic elements:**
1. **ACC Model Coordination Workflow** (Table 9-1) — 5-step workflow
2. **Coordination Cycle** (Table 9-2) — 5 activities with responsible party, frequency, trigger
3. **Federation Schedule** (Table 9-3) — 6 activities with frequency, day/time, location
4. **Coordination by Project Stage** (Table 9-4) — 4 stages (A3–A6) with federation frequency, submission day, review day
5. **Weekly Coordination Schedule** (Table 9-5) — 7 activities
6. **Clash Detection Responsibilities** (Table 9-6) — Named individuals with role and 5 responsibility columns
7. **Clash Detection Methodology** — Hard clashes (0mm tolerance), soft clashes (per EIR)
8. **Clash Detection Rulesets** (9.2.3) — Three severity categories:
   - Category A Critical (Table 9-7): 6 rulesets
   - Category B Significant (Table 9-8): 5 rulesets
   - Category C Minor (Table 9-9): 4 rulesets
   - Each with ID, discipline pairing, description, tolerance
9. Additional notes on rulesets (3 items)

**Quality observations:** The tiered clash detection ruleset with 15 named rulesets across 3 severity categories is extremely professional and operationally useful. This is one of the strongest sections in the entire Benchmark.

---

### Section 10 — Implementation of IPMP (pp. 61–72)

**Atomic elements:**
1. **Information Delivery Workflows** (10.1) — Figure 10-1 (swimlane flowchart S1→S2→S3→S5 across Task Team / Partner CDE / National Grid ACC)
2. **S3 Workflow changes** (10.2) — 10 bullet points detailing progressive assurance derogation; Figure 10-2 (S3 engineering and non-engineering flowchart)
3. **Transmittals** (10.3) — Table 10-1 (transmittal roles by scope with Doc Control and PkIM contacts)
4. **Drawing types** (10.4.1) — 3 types (newly created, pre-existing as-built, generic) with detailed rules
5. **Drawing sheet templates** (10.4.2) — Table 10-2 (8 templates), Figure 10-3 (title block example)
6. **Modifying As-built Information** (10.5) — 3 methods (revision, supersession, obsolescence):
   - 10.5.1: Revision procedure with 6 conditions
   - 10.5.2: Supersession procedure with 5 steps and figure
   - 10.5.3: Obsolescence procedure with 4 steps and figure
   - 10.5.4: Assurance table for modified drawings (revised/superseded/obsolete × DCAAR/ECM)
7. **Security of information** (10.6) — BS ISO 19650-5:2020 reference:
   - 10.6.1.1: 12 baseline security requirements
   - 10.6.1.2: Interim governance position
   - 10.6.1.3: PIM security control
   - 10.6.1.4: RACI for security (interim)
   - 10.6.1.5: Incident management with phone/email contacts and partner escalation table (Table 10-3)
   - 10.6.1.6: Monitoring and assurance
   - 10.6.1.7: SMP transition plan
8. **Legacy information** (10.7) — SPIM responsibilities (5 items), CDE location, 4 agreed release workflows
9. **Survey Strategy** (10.8) — Table 10-4 (4 survey types with format, grid, comments), BS 5964-3 reference
10. **RFI/TQ Process** (10.9) — ACC module reference, partner-only restriction
11. **Partner-to-Partner Communications** (10.10) — Custom issues template with 8 creator fields and 2 assignee fields

**Quality observations:** The security section (10.6) is the most comprehensive I've seen in a BEP — it covers baseline controls, governance, PIM-specific controls, RACI, incident management with actual contact numbers, monitoring, and SMP transition. The as-built modification procedures with red/green markup figures are highly practical.

---

### Appendices (pp. 73–85)

**Atomic elements:**
1. **Appendix A** — 14 abbreviations + 23 glossary terms (ISO 19650 aligned definitions)
2. **Appendix B** — Standards applicability matrix (16 standards × 16 application areas with M/R ratings) + 12 detailed standard abstracts
3. **Appendix C** — Mobilisation checklist (34 tasks across 6 categories: A-DTCCA, B-CDE, C-IT, D-Standards, E-Capacity, F-Education)
4. **Appendix D** — PIM Information Breakdown Strategy with hierarchical diagram (Project PIM → Asset → Package → Containers)
5. **Appendix E** — Project Organisation and Contact Information

---

## Summary Statement


### Key Strengths of the Benchmark That Define the Gold Standard:

1. **Governance maturity** — RACI review record for the BEP itself, change management triggers, exceptions/derogations with risk register traceability
2. **Role granularity** — 14 distinct IM functions with 5–25 individual responsibilities each, security clearance tracking
3. **Information delivery depth** — Separate high-level responsibility matrices per substation, 15 individual TIDPs, dual training plans
4. **Federation excellence** — 8-step process, swimlane flowchart, Model Register template, geolocation verification requirements
5. **PIM/AIM completeness** — Progressive assurance pipeline, 3-tier security on exchanges, 6-category AIM validation criteria, distributed CDE diagram
6. **Technical precision** — Per-software standardisation (Revit 5 tables, AutoCAD 3 tables, PLS-CADD 3 tables), 3 coordinate systems, 9-unit definition table
7. **Clash detection rigour** — 15 tiered rulesets across 3 severity categories, named responsibilities, stage-based coordination frequency
8. **Security comprehensiveness** — BS ISO 19650-5 compliance, 12 baseline requirements, incident contacts, SMP transition plan
9. **Operational procedures** — S3 progressive assurance workflow, 3 as-built modification methods with visual examples, transmittal roles by scope
10. **Appendix quality** — 34-task mobilisation checklist, full standards applicability matrix, PIM breakdown strategy diagram

