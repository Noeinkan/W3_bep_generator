version: "3.9"

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    env_file:
      - .env.production
    environment:
      NODE_ENV: production
      PORT: 3001
      ML_SERVICE_URL: http://ml-service:8000
    volumes:
      - sqlite-data:/app/server/db
    depends_on:
      - ml-service
    networks:
      - internal
    restart: unless-stopped

  ml-service:
    build:
      context: ./ml-service
      dockerfile: Dockerfile
    env_file:
      - .env.production
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL}
      OLLAMA_TIMEOUT: ${OLLAMA_TIMEOUT}
      OLLAMA_DEFAULT_TEMPERATURE: ${OLLAMA_DEFAULT_TEMPERATURE}
    depends_on:
      - ollama
    networks:
      - internal
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - internal
    restart: unless-stopped

  nginx:
    image: nginx:1.25-alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf:ro
      - /etc/letsencrypt:/etc/letsencrypt:ro
      - /var/www/certbot:/var/www/certbot:ro
    depends_on:
      - backend
    networks:
      - internal
    restart: unless-stopped

volumes:
  sqlite-data:
  ollama-data:

networks:
  internal:
    driver: bridge
